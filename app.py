import streamlit as st
import pandas as pd
import numpy as np
import requests
import joblib
import plotly.graph_objects as go
import os
from datetime import datetime, timedelta, time as dt_time

# [Added] è‡ªå‹•åˆ·æ–°å¥—ä»¶
try:
    from streamlit_autorefresh import st_autorefresh
except ImportError:
    st_autorefresh = None

# [Added] å¼•å…¥ PyGithub
try:
    from github import Github, Auth, GithubException
except ImportError:
    pass 

# ==========================================
# 1. ç¶²é è¨­å®šèˆ‡å…¨åŸŸåƒæ•¸
# ==========================================
st.set_page_config(
    page_title="AI äº¤æ˜“è¨Šè™Ÿæˆ°æƒ…å®¤ (Pro)", 
    layout="wide", 
    initial_sidebar_state="expanded",
    page_icon="ğŸ“ˆ"
)

# CSS ç¾åŒ–
st.markdown("""
    <style>
        .block-container {
            padding-top: 1.5rem !important; 
            padding-bottom: 3rem;
            max-width: 98% !important;
        }
        div[data-testid="stMetricValue"] {
            font-size: 20px;
            font-weight: bold;
        }
        .stButton button {
            width: 100%;
            border-radius: 8px;
            font-weight: 600;
        }
        div[data-testid="stDataFrame"] {
            font-family: 'Consolas', 'Monaco', monospace;
        }
    </style>
""", unsafe_allow_html=True)

# ------------------------------------------------------------------
# [é‡è¦] çµç®—æ—¥è¨­å®š
# ------------------------------------------------------------------
SETTLEMENT_DATES = {
    # 2025
    '2025-01-15', '2025-02-19', '2025-03-19', '2025-04-16', '2025-05-21', '2025-06-18',
    '2025-07-16', '2025-08-20', '2025-09-17', '2025-10-15', '2025-11-19', '2025-12-17',
    # 2026
    '2026-01-21', '2026-02-18', '2026-03-18', '2026-04-15', '2026-05-20', '2026-06-17',
    '2026-07-15', '2026-08-19', '2026-09-16', '2026-10-21', '2026-11-18', '2026-12-16'
}

# è³‡æ–™åº«è·¯å¾‘
HIST_FILE_DAY = 'history_data_day.csv'
HIST_FILE_FULL = 'history_data_full.csv'

# Session State
if 'df_view' not in st.session_state: st.session_state.df_view = pd.DataFrame()
if 'entry_idx' not in st.session_state: st.session_state.entry_idx = -1
if 'current_mode' not in st.session_state: st.session_state.current_mode = None 
if 'last_update' not in st.session_state: st.session_state.last_update = None
if 'data_range_info' not in st.session_state: st.session_state.data_range_info = ""

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½: è³‡æ–™æŠ“å–èˆ‡è¨ˆç®—
# ==========================================
class DataEngine:
    def __init__(self):
        self.feature_cols = [
            'Bandwidth', 'MA_Slope', 'Bandwidth_Rate', 'Rel_Volume',
            'K', 'D', 'Position_in_Channel', 'Volatility', 
            'K_Strength', 'Body_Ratio', 'Week', 'Settlement_Day', 'Time_Segment'
        ]
        self.exit_feature_cols = self.feature_cols + ['Floating_PnL', 'Hold_Bars']

    def _parse_anue_response(self, data):
        if not data.get('t'): return pd.DataFrame()
        try:
            df = pd.DataFrame({
                'Time': pd.to_datetime(data['t'], unit='s'),
                'Open': data['o'], 'High': data['h'], 'Low': data['l'], 'Close': data['c'], 'Volume': data['v']
            })
            df['Time'] = df['Time'].dt.tz_localize('UTC').dt.tz_convert('Asia/Taipei').dt.tz_localize(None)
            df['Time'] = df['Time'] + timedelta(minutes=5)
            
            cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')
            df = df.dropna(subset=cols)
            return df
        except Exception as e:
            st.warning(f"è³‡æ–™è§£æç•°å¸¸: {e}")
            return pd.DataFrame()

    def fetch_anue_raw(self):
        symbol = "TWF:TXF:FUTURES"
        url = "https://ws.api.cnyes.com/ws/api/v1/charting/history"
        headers = {"User-Agent": "Mozilla/5.0", "Referer": f"https://stock.cnyes.com/market/{symbol}"}
        
        to_ts = int(datetime.now().timestamp())
        params = {"symbol": symbol, "resolution": "5", "to": to_ts, "limit": 1000}
        
        try:
            res = requests.get(url, params=params, headers=headers, timeout=8)
            if res.status_code == 200:
                data = res.json().get('data', {})
                return self._parse_anue_response(data)
        except Exception as e:
            st.error(f"é‰…äº¨ç¶²é€£ç·šéŒ¯èª¤: {e}")
        return pd.DataFrame()

    def merge_and_save(self, api_df, hist_file, is_day_mode=False):
        # è®€å–æ­·å²è³‡æ–™
        hist_df = pd.DataFrame()
        if os.path.exists(hist_file):
            try:
                hist_df = pd.read_csv(hist_file)
                hist_df['Time'] = pd.to_datetime(hist_df['Time'])
            except: pass

        # éæ¿¾æ–° API è³‡æ–™
        new_df = api_df.copy()
        if not new_df.empty and is_day_mode:
            new_df = new_df.set_index('Time').sort_index()
            new_df = new_df.between_time(dt_time(8, 45), dt_time(13, 45)).reset_index()

        # åˆä½µé‚è¼¯ (ç¢ºä¿æ­·å²é€£çºŒæ€§)
        if not new_df.empty:
            if not hist_df.empty:
                full_df = pd.concat([hist_df, new_df])
            else:
                full_df = new_df
            
            # é‡è¦ï¼šåˆä½µå¾Œå…ˆæ’åºï¼Œç¢ºä¿æ™‚é–“åºåˆ—æ­£ç¢ºï¼Œé€™å°å¾ŒçºŒè¨ˆç®—è‡³é—œé‡è¦
            full_df = full_df.sort_values('Time')
            full_df = full_df.drop_duplicates(subset='Time', keep='last').reset_index(drop=True)
        else:
            full_df = hist_df

        # æ—¥ç›¤å†æ¬¡éæ¿¾é›œè¨Š
        if is_day_mode and not full_df.empty:
             full_df = full_df.set_index('Time').sort_index()
             full_df = full_df.between_time(dt_time(8, 45), dt_time(13, 45)).reset_index()

        # è‡ªå‹•æ¸…ç†ï¼šåªä¿ç•™æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥
        if not full_df.empty:
            full_df['Date'] = full_df['Time'].dt.date
            unique_dates = sorted(full_df['Date'].unique())
            if len(unique_dates) > 5:
                cutoff_date = unique_dates[-5]
                full_df = full_df[full_df['Date'] >= cutoff_date]
            full_df = full_df.drop(columns=['Date'])

        # å­˜æª”
        if not full_df.empty:
            full_df[['Time', 'Open', 'High', 'Low', 'Close', 'Volume']].to_csv(hist_file, index=False)
            
            start = full_df['Time'].iloc[0].strftime('%m/%d %H:%M')
            end = full_df['Time'].iloc[-1].strftime('%m/%d %H:%M')
            days = len(unique_dates) if 'unique_dates' in locals() else '?'
            st.session_state.data_range_info = f"{start} ~ {end} (å…± {len(full_df)} K / {days} å¤©)"
        else:
            st.session_state.data_range_info = "å°šç„¡è³‡æ–™"

        return full_df

    def calculate_indicators(self, df, mode='day'):
        """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ (ç¢ºä¿ç„¡æœªä¾†æ•¸æ“šæ±™æŸ“)"""
        if df.empty: return df
        df = df.copy()
        
        # ç¢ºä¿æ™‚é–“åºåˆ—ç”±èˆŠåˆ°æ–°æ’åˆ—ï¼Œé€™å° Rolling è¨ˆç®—æ˜¯å¿…é ˆçš„
        df = df.sort_values('Time').reset_index(drop=True)
        
        C = df['Close']; H = df['High']; L = df['Low']; O = df['Open']; V = df['Volume']
        
        # æŒ‡æ¨™è¨ˆç®—
        # é€™è£¡æœƒç”¨åˆ°å‰é¢çš„æ­·å²è³‡æ–™ã€‚åªè¦ df è£¡åŒ…å«æ˜¨å¤©çš„è³‡æ–™ï¼Œä»Šå¤©çš„ MA å°±ä¸æœƒæ˜¯ NaN
        ma20 = C.rolling(20).mean()
        std20 = C.rolling(20).std()
        df['UB'] = ma20 + 2 * std20
        df['LB'] = ma20 - 2 * std20
        df['Bandwidth'] = df['UB'] - df['LB']
        
        df['MA_Slope'] = np.sign(ma20.diff()) 
        df['Bandwidth_Rate'] = df['Bandwidth'].pct_change()
        
        vol_ma = V.rolling(5).mean().replace(0, 1)
        df['Rel_Volume'] = V / vol_ma
        
        lowest_l = L.rolling(36).min()
        highest_h = H.rolling(36).max()
        denom = (highest_h - lowest_l).replace(0, 1)
        rsv = (C - lowest_l) / denom
        
        df['K'] = rsv.ewm(alpha=1/3, adjust=False).mean()
        df['D'] = df['K'].ewm(alpha=1/3, adjust=False).mean()
        
        bw_safe = df['Bandwidth'].replace(0, 0.0001)
        df['Position_in_Channel'] = (C - df['LB']) / bw_safe
        
        df['Volatility'] = (H - L) / C * 100
        df['K_Strength'] = (C - O) / O * 100
        df['Body_Ratio'] = (C - O).abs() / (H - L).replace(0, 1)
        df['Week'] = df['Time'].dt.weekday + 1
        
        if mode == 'full':
            df['Settlement_Day'] = 0
            df['Time_Segment'] = 1
        else:
            df['Settlement_Day'] = df['Time'].apply(
                lambda t: 1 if (t.weekday() in [2,4] or str(t.date()) in SETTLEMENT_DATES) else 0
            )
            hm = df['Time'].dt.hour * 100 + df['Time'].dt.minute
            df['Time_Segment'] = np.select([hm <= 930, hm <= 1200], [0, 1], default=2)
        
        # [é‚è¼¯ç¢ºèª]
        # ä½¿ç”¨ fillna(0) æ˜¯ç‚ºäº†è™•ç†ã€Œè³‡æ–™é›†æœ€é–‹é ­ã€çš„ NaN (5å¤©å‰çš„è³‡æ–™)ã€‚
        # å› ç‚ºæˆ‘å€‘åœ¨è¨ˆç®—å‰å·²ç¶“è¼‰å…¥äº†å®Œæ•´çš„æ­·å²è³‡æ–™ (df åŒ…å« 5 å¤©)ï¼Œ
        # æ‰€ä»¥ã€Œä»Šå¤© 08:45ã€çš„è³‡æ–™å‰é¢å·²ç¶“æœ‰ã€Œæ˜¨å¤©ã€çš„è³‡æ–™åšæ”¯æ’ï¼Œ
        # è¨ˆç®—å‡ºä¾†çš„ MA, KD ç­‰æŒ‡æ¨™æœƒæ˜¯æœ‰æ•ˆå€¼ï¼Œä¸æœƒè®Šæˆ 0ã€‚
        # é€™æ¨£æ—¢é¿å…äº†ã€Œå·çœ‹æœªä¾† (bfill)ã€ï¼Œä¹Ÿç¢ºä¿äº†ã€Œä»Šæ—¥è¨ˆç®—é€£çºŒæ€§ã€ã€‚
        df[self.feature_cols] = df[self.feature_cols].fillna(0)
        return df

# ==========================================
# 3. ç­–ç•¥å¼•æ“
# ==========================================
class StrategyEngine:
    def __init__(self, models, params, df):
        self.models = models
        self.params = params
        self.df = df
        self.processor = DataEngine()

    def find_entry_info(self, entry_time_obj):
        if entry_time_obj is None: return -1, 0.0
        time_str = entry_time_obj.strftime("%H:%M")
        matches = self.df[self.df['Time'].astype(str).str.contains(time_str, na=False)]
        if not matches.empty:
            return matches.index[-1], matches.iloc[-1]['Close']
        return -1, 0.0

    def run_analysis(self, user_pos_type, entry_time_obj):
        """
        åŸ·è¡Œç­–ç•¥åˆ†æ
        é‚è¼¯èªªæ˜ï¼š
        1. Batch Prediction (æ‰¹æ¬¡é æ¸¬) æ˜¯ç‚ºäº†åŠ é€Ÿã€‚
        2. å› ç‚º Tree Model æ˜¯ Stateless (ç„¡ç‹€æ…‹) çš„ï¼ŒModel.predict(Row_N) çš„çµæœ
           åªå–æ±ºæ–¼ Row_N çš„ç‰¹å¾µã€‚
        3. Row_N çš„ç‰¹å¾µ (å¦‚ MA20) åœ¨ DataEngine éšæ®µå·²ç¶“è¨ˆç®—å®Œæˆï¼Œ
           å…¶æ•¸å€¼åƒ…åŒ…å« Row_0 åˆ° Row_N çš„æ­·å²è³‡è¨Šã€‚
        4. å› æ­¤ï¼Œä¸€æ¬¡ç®—å®Œæ‰€æœ‰æ©Ÿç‡ï¼Œèˆ‡è¿´åœˆä¸­é€ç­†è¨ˆç®—ï¼Œæ•¸å­¸çµæœå®Œå…¨ç›¸åŒï¼Œä¸”ç„¡æœªä¾†è¦–å•é¡Œã€‚
        """
        if self.df.empty: return pd.DataFrame(), -1
        
        X_all = self.df[self.processor.feature_cols]
        
        # --- Step 1: æ‰¹æ¬¡é æ¸¬ (å¿«é€Ÿç®—å‡ºæ¯æ ¹ K æ£’çš„åŸå§‹è¨Šè™Ÿ) ---
        try:
            # é€™è£¡è¨ˆç®—å‡ºä¾†çš„ probs_long[i] ä»£è¡¨ï¼š
            # åœ¨ç¬¬ i æ ¹ K æ£’çµæŸç•¶ä¸‹ (åŒ…å«äº† 0~i çš„æ­·å²ç‰¹å¾µ)ï¼ŒAI å°æœªä¾†çš„åˆ¤æ–·
            probs_long = self.models['Long_Entry_Model'].predict_proba(X_all)[:, 1]
            probs_short = self.models['Short_Entry_Model'].predict_proba(X_all)[:, 1]
        except:
            probs_long = np.zeros(len(self.df))
            probs_short = np.zeros(len(self.df))

        # --- Step 2: æº–å‚™ä½¿ç”¨è€…æŒå€‰è³‡è¨Š ---
        pos_map = {"ç©ºæ‰‹ (Empty)": "Empty", "å¤šå–® (Long)": "Long", "ç©ºå–® (Short)": "Short"}
        u_pos = pos_map.get(user_pos_type, "Empty")
        user_entry_idx, user_cost = self.find_entry_info(entry_time_obj) if u_pos != "Empty" else (-1, 0.0)
        
        # --- Step 3: ç­–ç•¥ç‹€æ…‹æ©Ÿè¿´åœˆ (æ¨¡æ“¬æ™‚é–“æ¨æ¼”) ---
        history_records = []
        
        s_pos = 0     # ç­–ç•¥æŒå€‰
        s_price = 0.0 # é€²å ´åƒ¹
        s_idx = 0     # é€²å ´Index
        
        # é€ç­†æ¨¡æ“¬ï¼Œç¢ºä¿æ¯ä¸€ç­†äº¤æ˜“æ±ºç­–éƒ½åªåŸºæ–¼ç•¶ä¸‹æˆ–éå»çš„ç‹€æ…‹
        for i in range(len(self.df)):
            curr_row = self.df.iloc[i]
            
            # å–å‡ºç•¶ä¸‹æ™‚é–“é» AI çš„åˆ¤æ–· (é€™å€‹æ©Ÿç‡å€¼åªåŒ…å« <= i æ™‚é–“é»çš„è³‡è¨Š)
            p_long = probs_long[i]
            p_short = probs_short[i]
            
            trend_str = f"(å¤š:{p_long:.0%}/ç©º:{p_short:.0%})"
            s_action, s_detail = "âšª è§€æœ›", trend_str
            
            # ç­–ç•¥é€²å‡ºå ´é‚è¼¯ (ç‹€æ…‹æ©Ÿ)
            if s_pos == 0:
                if p_long > self.params['entry'] and p_long > p_short:
                    s_pos = 1; s_price = curr_row['Close']; s_idx = i
                    s_action = "ğŸ”´ è²·é€²"; s_detail = f"å¤š {p_long:.0%} {trend_str}"
                elif p_short > self.params['entry'] and p_short > p_long:
                    s_pos = -1; s_price = curr_row['Close']; s_idx = i
                    s_action = "ğŸŸ¢ æ”¾ç©º"; s_detail = f"ç©º {p_short:.0%} {trend_str}"
            
            elif s_pos == 1: # æŒæœ‰å¤šå–®
                pnl = curr_row['Close'] - s_price
                if pnl <= -self.params['stop']:
                    s_pos = 0; s_action = "ğŸ’¥ åœæ"; s_detail = f"æ {pnl:.0f}"
                else:
                    # å‡ºå ´ç‰¹å¾µéœ€è¦åŒ…å«ç•¶ä¸‹çš„ PnLï¼Œæ‰€ä»¥é€™è£¡éœ€å³æ™‚çµ„å»ºç‰¹å¾µ
                    # é€™é‚Šåªé‡å°å–®ç­†è³‡æ–™é æ¸¬ï¼Œä¹Ÿä¸æœƒå·çœ‹æœªä¾†
                    row_feats = X_all.iloc[[i]].copy()
                    row_feats['Floating_PnL'] = pnl
                    row_feats['Hold_Bars'] = i - s_idx
                    ep = self.models['Long_Exit_Model'].predict_proba(row_feats[self.processor.exit_feature_cols])[0][1]
                    
                    if ep > self.params['exit']:
                        s_pos = 0; s_action = "âŒ å¤šå‡º"; s_detail = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"
                    else:
                        s_action = "â³ çºŒæŠ±"; s_detail = f"å¸³{pnl:.0f}(çºŒ:{1-ep:.0%})"

            elif s_pos == -1: # æŒæœ‰ç©ºå–®
                pnl = s_price - curr_row['Close']
                if pnl <= -self.params['stop']:
                    s_pos = 0; s_action = "ğŸ’¥ åœæ"; s_detail = f"æ {pnl:.0f}"
                else:
                    row_feats = X_all.iloc[[i]].copy()
                    row_feats['Floating_PnL'] = pnl
                    row_feats['Hold_Bars'] = i - s_idx
                    ep = self.models['Short_Exit_Model'].predict_proba(row_feats[self.processor.exit_feature_cols])[0][1]
                    
                    if ep > self.params['exit']:
                        s_pos = 0; s_action = "â ç©ºå‡º"; s_detail = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"
                    else:
                        s_action = "â³ çºŒæŠ±"; s_detail = f"å¸³{pnl:.0f}(çºŒ:{1-ep:.0%})"

            # çœŸå¯¦æŒå€‰å»ºè­° (User Advice) - é‚è¼¯åŒä¸Šï¼Œç•¥ç‚ºç²¾ç°¡
            u_action, u_note = "-", "-"
            if u_pos != "Empty" and i >= user_entry_idx:
                hold_bars = i - user_entry_idx
                if u_pos == "Long":
                    pnl = curr_row['Close'] - user_cost
                    if i == user_entry_idx: u_action, u_note = "ğŸ”´ å¤šå–®é€²å ´", f"æœ¬ {user_cost:.0f}"
                    elif pnl <= -self.params['stop']: u_action, u_note = "ğŸ’¥ åœæ", f"{pnl:.0f}"
                    else:
                        row_feats = X_all.iloc[[i]].copy()
                        row_feats['Floating_PnL'] = pnl; row_feats['Hold_Bars'] = hold_bars
                        ep = self.models['Long_Exit_Model'].predict_proba(row_feats[self.processor.exit_feature_cols])[0][1]
                        u_action = "âŒ å‡ºå ´" if ep > self.params['exit'] else ("ğŸ”¥ åŠ ç¢¼" if p_long > self.params['entry'] else "â³ çºŒæŠ±")
                        u_note = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"
                elif u_pos == "Short":
                    pnl = user_cost - curr_row['Close']
                    if i == user_entry_idx: u_action, u_note = "ğŸŸ¢ ç©ºå–®é€²å ´", f"æœ¬ {user_cost:.0f}"
                    elif pnl <= -self.params['stop']: u_action, u_note = "ğŸ’¥ åœæ", f"{pnl:.0f}"
                    else:
                        row_feats = X_all.iloc[[i]].copy()
                        row_feats['Floating_PnL'] = pnl; row_feats['Hold_Bars'] = hold_bars
                        ep = self.models['Short_Exit_Model'].predict_proba(row_feats[self.processor.exit_feature_cols])[0][1]
                        u_action = "â å‡ºå ´" if ep > self.params['exit'] else ("ğŸ”¥ åŠ ç¢¼" if p_short > self.params['entry'] else "â³ çºŒæŠ±")
                        u_note = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"

            history_records.append({
                'Time': curr_row['Time'], 'Close': curr_row['Close'],
                'UB': curr_row.get('UB', np.nan), 'LB': curr_row.get('LB', np.nan),
                'Strategy_Action': s_action, 'Strategy_Detail': s_detail,
                'User_Advice': u_action, 'User_Note': u_note
            })
            
        return pd.DataFrame(history_records), user_entry_idx

# ==========================================
# 4. GitHub å­˜æª”åŠŸèƒ½
# ==========================================
def push_to_github(file_path, df_to_save):
    token = st.secrets.get("GITHUB_TOKEN")
    repo_name = st.secrets.get("GITHUB_REPO")
    if not token or not repo_name: return "âŒ ç¼ºå°‘ GitHub è¨­å®š"
    if "/" not in repo_name: return f"âŒ Repo åç¨±éŒ¯èª¤: '{repo_name}'"

    try:
        g = Github(token)
        repo = g.get_repo(repo_name)
        csv_content = df_to_save.to_csv(index=False)
        try:
            contents = repo.get_contents(file_path)
            repo.update_file(contents.path, f"Update {file_path}", csv_content, contents.sha)
            return "âœ… é›²ç«¯æ›´æ–°æˆåŠŸï¼"
        except:
            repo.create_file(file_path, f"Create {file_path}", csv_content)
            return "âœ… é›²ç«¯å»ºç«‹æˆåŠŸï¼"
    except GithubException as e:
        if e.status == 404: return "âŒ 404 éŒ¯èª¤ (Repo ä¸å­˜åœ¨æˆ–ç„¡æ¬Šé™)"
        return f"âŒ GitHub éŒ¯èª¤: {e.data.get('message', str(e))}"
    except Exception as e: return f"âŒ æœªçŸ¥éŒ¯èª¤: {str(e)}"

# ==========================================
# 5. UI ä¸»ç¨‹å¼
# ==========================================
@st.cache_resource
def load_models():
    models = {}
    req = ['Long_Entry_Model', 'Short_Entry_Model', 'Long_Exit_Model', 'Short_Exit_Model']
    missing = []
    for name in req:
        p1, p2 = f"models/{name}.pkl", f"{name}.pkl"
        if os.path.exists(p1): models[name] = joblib.load(p1)
        elif os.path.exists(p2): models[name] = joblib.load(p2)
        else: missing.append(name)
    if missing: st.error(f"âŒ ç¼ºå°‘æ¨¡å‹: {missing}"); return None
    return models

if st_autorefresh: st_autorefresh(interval=300000, key="auto_refresh")

engine = DataEngine()
models = load_models()

with st.sidebar:
    st.header("ğŸ® æ§åˆ¶å°")
    col_day, col_full = st.columns(2)
    trigger_day = col_day.button("ğŸŒ æ›´æ–°æ—¥ç›¤", type="primary", use_container_width=True)
    trigger_full = col_full.button("ğŸŒ™ æ›´æ–°å…¨ç›¤", use_container_width=True)
    
    if st.button("ğŸ§¹ é‡ç½®è³‡æ–™åº«"):
        if os.path.exists(HIST_FILE_DAY): os.remove(HIST_FILE_DAY)
        if os.path.exists(HIST_FILE_FULL): os.remove(HIST_FILE_FULL)
        st.cache_data.clear()
        st.session_state.df_view = pd.DataFrame()
        st.rerun()

    with st.expander("âš™ï¸ åƒæ•¸èˆ‡éƒ¨ä½", expanded=True):
        p_entry = st.slider("é€²å ´ä¿¡å¿ƒ", 0.5, 0.95, 0.80, 0.05)
        p_exit = st.slider("å‡ºå ´æ©Ÿç‡", 0.3, 0.9, 0.50, 0.05)
        p_stop = st.number_input("ç¡¬åœæ", 50, 500, 100, step=10)
        st.markdown("---")
        u_pos = st.radio("çœŸå¯¦æŒå€‰", ["ç©ºæ‰‹ (Empty)", "å¤šå–® (Long)", "ç©ºå–® (Short)"])
        u_time = st.time_input("é€²å ´æ™‚é–“", value=dt_time(9,0), step=300) if u_pos != "ç©ºæ‰‹ (Empty)" else None

    with st.expander("ğŸ’¾ è³‡æ–™åº«ç®¡ç†", expanded=False):
        st.caption("æ‰‹å‹•ç¶­è­·èˆ‡é›²ç«¯åŒæ­¥")
        tab_d, tab_f = st.tabs(["æ—¥ç›¤", "å…¨ç›¤"])
        with tab_d:
            up_day = st.file_uploader("ä¸Šå‚³æ—¥ç›¤", type=['csv'], key="up_day")
            if up_day: pd.read_csv(up_day).to_csv(HIST_FILE_DAY, index=False); st.success("æ›´æ–°æ—¥ç›¤")
            if st.button("ä¸Šå‚³ GitHub (æ—¥)", key="gd"):
                if os.path.exists(HIST_FILE_DAY): 
                    with st.spinner("ä¸Šå‚³ä¸­..."): st.write(push_to_github(HIST_FILE_DAY, pd.read_csv(HIST_FILE_DAY)))
        with tab_f:
            up_full = st.file_uploader("ä¸Šå‚³å…¨ç›¤", type=['csv'], key="up_full")
            if up_full: pd.read_csv(up_full).to_csv(HIST_FILE_FULL, index=False); st.success("æ›´æ–°å…¨ç›¤")
            if st.button("ä¸Šå‚³ GitHub (å…¨)", key="gf"):
                if os.path.exists(HIST_FILE_FULL): 
                    with st.spinner("ä¸Šå‚³ä¸­..."): st.write(push_to_github(HIST_FILE_FULL, pd.read_csv(HIST_FILE_FULL)))

def process_data(mode):
    hist_file = HIST_FILE_DAY if mode == 'day' else HIST_FILE_FULL
    api_df = engine.fetch_anue_raw()
    final_df = engine.merge_and_save(api_df, hist_file, is_day_mode=(mode=='day'))
    if final_df.empty: return pd.DataFrame(), "âŒ ç„¡è³‡æ–™"
    status = "OK" if not api_df.empty else "âš ï¸ API ç„¡æ–°è³‡æ–™"
    return engine.calculate_indicators(final_df, mode=mode), status

if trigger_day:
    with st.spinner("æ•´åˆæ—¥ç›¤..."):
        df_res, status = process_data('day')
        st.session_state.df_view = df_res; st.session_state.current_mode = 'day'
        st.session_state.last_update = datetime.now()
        if status != "OK": st.toast(status, icon="âš ï¸")

if trigger_full:
    with st.spinner("æ•´åˆå…¨ç›¤..."):
        df_res, status = process_data('full')
        st.session_state.df_view = df_res; st.session_state.current_mode = 'full'
        st.session_state.last_update = datetime.now()
        if status != "OK": st.toast(status, icon="âš ï¸")

if not st.session_state.df_view.empty and models:
    icon = "ğŸŒ" if st.session_state.current_mode == 'day' else "ğŸŒ™"
    st.title(f"{icon} æˆ°æƒ…å®¤")
    c1, c2 = st.columns([3, 1])
    c1.info(st.session_state.data_range_info)
    if st.session_state.last_update: c2.caption(f"æ›´æ–°: {st.session_state.last_update.strftime('%H:%M:%S')}")

    strat = StrategyEngine(models, {'entry': p_entry, 'exit': p_exit, 'stop': p_stop}, st.session_state.df_view)
    df_display, entry_idx = strat.run_analysis(u_pos, u_time)
    
    last = df_display.iloc[-1]
    m1, m2, m3 = st.columns(3)
    m1.metric("åƒ¹æ ¼", f"{last['Close']:.0f}")
    m2.metric("ç­–ç•¥", last['Strategy_Action'])
    m3.metric("ä¿¡å¿ƒ", last['Strategy_Detail'].split('(')[-1].replace(')', ''))

    df_chart = df_display.copy()
    df_chart['Time_Str'] = df_chart['Time'].dt.strftime('%H:%M')
    total_len = len(df_chart)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_chart['Time_Str'], y=df_chart['UB'], mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip'))
    fig.add_trace(go.Scatter(x=df_chart['Time_Str'], y=df_chart['LB'], mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(173, 216, 230, 0.2)', name='BB'))
    fig.add_trace(go.Scatter(x=df_chart['Time_Str'], y=df_chart['Close'], mode='lines', name='Price', line=dict(color='#1f77b4', width=2)))
    
    for act, sym, col, nm in [('è²·é€²', 'triangle-up', 'red', 'Buy'), ('æ”¾ç©º', 'triangle-down', 'green', 'Sell'), ('å‡º', 'x', 'gray', 'Exit')]:
        mask = df_chart['Strategy_Action'].str.contains(act)
        if mask.any():
            sub = df_chart[mask]
            fig.add_trace(go.Scatter(x=sub['Time'].dt.strftime('%H:%M'), y=sub['Close'], mode='markers', marker=dict(symbol=sym, size=12, color=col), name=nm))

    if entry_idx != -1 and entry_idx in df_chart.index:
        r = df_chart.loc[entry_idx]
        fig.add_trace(go.Scatter(x=[r['Time_Str']], y=[r['Close']], mode='markers', marker=dict(symbol='star', size=18, color='gold', line=dict(width=1, color='black')), name='My Entry'))

    fig.update_layout(height=550, margin=dict(t=30,l=10,r=10,b=10), xaxis=dict(type='category', rangeslider=dict(visible=True), range=[max(0, total_len-150), total_len-1]), legend=dict(orientation="h", y=1.02, x=0.5, xanchor="center"), hovermode="x unified")
    st.plotly_chart(fig, use_container_width=True)
    
    st.subheader("ğŸ“œ è¨Šè™Ÿå±¥æ­·")
    st.dataframe(df_display.iloc[::-1], height=400, use_container_width=True, hide_index=True)
    
elif models is None: st.warning("âš ï¸ ç¼ºå°‘æ¨¡å‹æª”æ¡ˆ")
else: st.info("ğŸ‘ˆ è«‹é»æ“Šå·¦å´æ›´æ–°æŒ‰éˆ•")

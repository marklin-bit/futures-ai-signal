import streamlit as st
import pandas as pd
import numpy as np
import requests
import joblib
import plotly.graph_objects as go
import os
from datetime import datetime, timedelta, time as dt_time

# [Added] è‡ªå‹•åˆ·æ–°å¥—ä»¶
try:
    from streamlit_autorefresh import st_autorefresh
except ImportError:
    st_autorefresh = None

# [Added] å¼•å…¥ PyGithub
try:
    from github import Github, Auth, GithubException
except ImportError:
    pass 

# ==========================================
# 1. ç¶²é è¨­å®šèˆ‡å…¨åŸŸåƒæ•¸
# ==========================================
st.set_page_config(page_title="AI äº¤æ˜“è¨Šè™Ÿæˆ°æƒ…å®¤ (Pro)", layout="wide", initial_sidebar_state="expanded")

# CSS ç¾åŒ–
st.markdown("""
    <style>
        .block-container {
            padding-top: 2rem !important; 
            padding-bottom: 5rem;
            max-width: 98% !important;
        }
        section[data-testid="stSidebar"] .block-container {
            padding-top: 2rem;
        }
        div[data-testid="stMetricValue"] {font-size: 24px;}
        .stButton button {
            width: 100%;
            border-radius: 5px;
            font-weight: bold;
        }
    </style>
""", unsafe_allow_html=True)

# 2026 å¹´æœˆçµç®—æ—¥æ¸…å–®
SETTLEMENT_DATES_2026 = {
    '2026-01-21', '2026-02-18', '2026-03-18', '2026-04-15', '2026-05-20', '2026-06-17',
    '2026-07-15', '2026-08-19', '2026-09-16', '2026-10-21', '2026-11-18', '2026-12-16'
}

# å®šç¾©å…©å€‹ç¨ç«‹çš„è³‡æ–™åº«æª”æ¡ˆ
HIST_FILE_DAY = 'history_data_day.csv'   # ç´”æ—¥ç›¤è³‡æ–™åº«
HIST_FILE_FULL = 'history_data_full.csv' # å…¨ç›¤è³‡æ–™åº«

# Session State åˆå§‹åŒ–
if 'df_view' not in st.session_state: st.session_state.df_view = pd.DataFrame()
if 'entry_idx' not in st.session_state: st.session_state.entry_idx = -1
if 'current_mode' not in st.session_state: st.session_state.current_mode = None 
if 'last_update' not in st.session_state: st.session_state.last_update = None
if 'data_range_info' not in st.session_state: st.session_state.data_range_info = ""

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½: è³‡æ–™æŠ“å–èˆ‡è¨ˆç®—
# ==========================================
class DataEngine:
    def __init__(self):
        self.feature_cols = [
            'Bandwidth', 'MA_Slope', 'Bandwidth_Rate', 'Rel_Volume',
            'K', 'D', 'Position_in_Channel', 'Volatility', 
            'K_Strength', 'Body_Ratio', 'Week', 'Settlement_Day', 'Time_Segment'
        ]
        self.exit_feature_cols = self.feature_cols + ['Floating_PnL', 'Hold_Bars']

    def _parse_anue_response(self, data):
        """è§£æé‰…äº¨ç¶² API"""
        if not data.get('t'): return pd.DataFrame()
        df = pd.DataFrame({
            'Time': pd.to_datetime(data['t'], unit='s'),
            'Open': data['o'], 'High': data['h'], 'Low': data['l'], 'Close': data['c'], 'Volume': data['v']
        })
        # UTC -> Taiwan -> +5min (Kæ£’çµæŸæ™‚é–“)
        df['Time'] = df['Time'].dt.tz_localize('UTC').dt.tz_convert('Asia/Taipei').dt.tz_localize(None)
        df['Time'] = df['Time'] + timedelta(minutes=5)
        df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].apply(pd.to_numeric, errors='coerce')
        return df

    def fetch_anue_raw(self):
        """æŠ“å– API æ–°è³‡æ–™"""
        symbol = "TWF:TXF:FUTURES"
        url = "https://ws.api.cnyes.com/ws/api/v1/charting/history"
        headers = {"User-Agent": "Mozilla/5.0", "Referer": f"https://stock.cnyes.com/market/{symbol}"}
        
        to_ts = int(datetime.now().timestamp())
        # æŠ“å– 1000 ç­† (ç´„ 3-4 å¤©)
        params = {"symbol": symbol, "resolution": "5", "to": to_ts, "limit": 1000}
        
        try:
            res = requests.get(url, params=params, headers=headers, timeout=8)
            data = res.json().get('data', {})
            if data.get('t'):
                return self._parse_anue_response(data)
        except Exception as e:
            st.error(f"é‰…äº¨ç¶²é€£ç·šéŒ¯èª¤: {e}")
        
        return pd.DataFrame()

    def merge_and_save(self, api_df, hist_file, is_day_mode=False):
        """
        [å¢å¼·ç‰ˆ] åˆä½µã€éæ¿¾ã€å­˜æª”ã€ä¸¦è‡ªå‹•æ¸…ç†éæœŸè³‡æ–™ (åªç•™æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥)
        """
        # 1. è®€å–æ­·å²
        if os.path.exists(hist_file):
            try:
                hist_df = pd.read_csv(hist_file)
                hist_df['Time'] = pd.to_datetime(hist_df['Time'])
            except:
                hist_df = pd.DataFrame()
        else:
            hist_df = pd.DataFrame()

        # 2. è™•ç†æ–°è³‡æ–™
        new_df = api_df.copy()
        if not new_df.empty and is_day_mode:
            # æ—¥ç›¤æ¨¡å¼ï¼šåš´æ ¼éæ¿¾ï¼Œåªä¿ç•™ 08:45 ~ 13:45 çš„è³‡æ–™
            new_df = new_df.set_index('Time').sort_index()
            new_df = new_df.between_time(dt_time(8, 45), dt_time(13, 45)).reset_index()

        # 3. åˆä½µèˆ‡å»é‡
        if not new_df.empty:
            if not hist_df.empty:
                full_df = pd.concat([hist_df, new_df])
            else:
                full_df = new_df
            
            # ä¾æ™‚é–“å»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•¸æ“š
            full_df = full_df.drop_duplicates(subset='Time', keep='last').sort_values('Time').reset_index(drop=True)
        else:
            full_df = hist_df

        # ç¢ºä¿æ—¥ç›¤æ­·å²æª”ä¸å«é›œè³ª
        if is_day_mode and not full_df.empty:
             full_df = full_df.set_index('Time').sort_index()
             full_df = full_df.between_time(dt_time(8, 45), dt_time(13, 45)).reset_index()

        # 4. è‡ªå‹•æ¸…ç†ï¼šåªä¿ç•™æœ€è¿‘ 5 å€‹äº¤æ˜“æ—¥
        if not full_df.empty:
            unique_dates = full_df['Time'].dt.date.unique()
            unique_dates.sort()
            
            if len(unique_dates) > 5:
                cutoff_date = unique_dates[-5]
                full_df = full_df[full_df['Time'].dt.date >= cutoff_date]

        # 5. å­˜æª”
        if not full_df.empty:
            save_cols = ['Time', 'Open', 'High', 'Low', 'Close', 'Volume']
            full_df[save_cols].to_csv(hist_file, index=False)
            
            start_str = full_df['Time'].iloc[0].strftime('%Y-%m-%d %H:%M')
            end_str = full_df['Time'].iloc[-1].strftime('%Y-%m-%d %H:%M')
            st.session_state.data_range_info = f"{start_str} ~ {end_str} (å…± {len(full_df)} ç­† / {len(unique_dates) if 'unique_dates' in locals() else '?'} å¤©)"
        else:
            st.session_state.data_range_info = "ç„¡è³‡æ–™"

        return full_df

    def calculate_indicators(self, df, mode='day'):
        """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™"""
        if df.empty: return df
        df = df.sort_values('Time').reset_index(drop=True)
        
        C = df['Close']; H = df['High']; L = df['Low']; O = df['Open']; V = df['Volume']
        
        # 1. å¸ƒæ—é€šé“ (20, 2)
        ma20 = C.rolling(20).mean()
        std20 = C.rolling(20).std()
        df['UB'] = ma20 + 2 * std20
        df['LB'] = ma20 - 2 * std20
        df['Bandwidth'] = df['UB'] - df['LB']
        
        # 2. å…¶ä»–ç‰¹å¾µ
        df['MA_Slope'] = np.sign(ma20.diff()) 
        df['Bandwidth_Rate'] = df['Bandwidth'].pct_change()
        df['Rel_Volume'] = V / V.rolling(5).mean()
        
        # 3. KD (36, 3)
        rsv = (C - L.rolling(36).min()) / (H.rolling(36).max() - L.rolling(36).min())
        df['K'] = rsv.ewm(alpha=1/3, adjust=False).mean()
        df['D'] = df['K'].ewm(alpha=1/3, adjust=False).mean()
        
        df['Position_in_Channel'] = (C - df['LB']) / df['Bandwidth']
        df['Volatility'] = (H - L) / C * 100
        df['K_Strength'] = (C - O) / O * 100
        df['Body_Ratio'] = (C - O).abs() / (H - L).replace(0, 1)
        df['Week'] = df['Time'].dt.weekday + 1
        
        if mode == 'full':
            df['Settlement_Day'] = 0
            df['Time_Segment'] = 1
        else:
            df['Settlement_Day'] = df['Time'].apply(lambda t: 1 if (t.weekday() in [2,4] or str(t.date()) in SETTLEMENT_DATES_2026) else 0)
            hm = df['Time'].dt.hour * 100 + df['Time'].dt.minute
            df['Time_Segment'] = np.select([hm <= 930, hm <= 1200], [0, 1], default=2)
        
        df[self.feature_cols] = df[self.feature_cols].fillna(method='bfill').fillna(0)
        return df

# ==========================================
# 3. ç­–ç•¥å¼•æ“ (ç„¡è®Šæ›´)
# ==========================================
class StrategyEngine:
    def __init__(self, models, params, df):
        self.models = models
        self.params = params
        self.df = df
        self.processor = DataEngine()

    def find_entry_info(self, entry_time_obj):
        if entry_time_obj is None: return -1, 0.0
        time_str = entry_time_obj.strftime("%H:%M")
        matches = self.df[self.df['Time'].astype(str).str.contains(time_str, na=False)]
        if not matches.empty:
            return matches.index[-1], matches.iloc[-1]['Close']
        return -1, 0.0

    def run_analysis(self, user_pos_type, entry_time_obj):
        if self.df.empty: return pd.DataFrame(), {}
        
        history_records = []
        X_all = self.df[self.processor.feature_cols]
        pos_map = {"ç©ºæ‰‹ (Empty)": "Empty", "å¤šå–® (Long)": "Long", "ç©ºå–® (Short)": "Short"}
        u_pos = pos_map.get(user_pos_type, "Empty")
        user_entry_idx, user_cost = self.find_entry_info(entry_time_obj) if u_pos != "Empty" else (-1, 0.0)
        
        s_pos, s_price, s_idx = 0, 0.0, 0
        
        for i in range(len(self.df)):
            curr_row = self.df.iloc[i]
            curr_feats = X_all.iloc[[i]]
            
            try:
                p_long = self.models['Long_Entry_Model'].predict_proba(curr_feats)[0][1]
                p_short = self.models['Short_Entry_Model'].predict_proba(curr_feats)[0][1]
            except: p_long, p_short = 0.0, 0.0
            
            trend = f"(å¤š:{p_long:.0%}/ç©º:{p_short:.0%})"
            s_action, s_detail = "âšª è§€æœ›", trend
            
            if s_pos == 0:
                if p_long > self.params['entry'] and p_long > p_short:
                    s_pos, s_price, s_idx, s_action, s_detail = 1, curr_row['Close'], i, "ğŸ”´ è²·é€²", f"å¤š {p_long:.0%} {trend}"
                elif p_short > self.params['entry'] and p_short > p_long:
                    s_pos, s_price, s_idx, s_action, s_detail = -1, curr_row['Close'], i, "ğŸŸ¢ æ”¾ç©º", f"ç©º {p_short:.0%} {trend}"
            elif s_pos == 1:
                pnl = curr_row['Close'] - s_price
                if pnl <= -self.params['stop']:
                    s_pos, s_action, s_detail = 0, "ğŸ’¥ åœæ", f"æ {pnl:.0f}"
                else:
                    curr_feats_exit = curr_feats.assign(Floating_PnL=pnl, Hold_Bars=i-s_idx)
                    ep = self.models['Long_Exit_Model'].predict_proba(curr_feats_exit[self.processor.exit_feature_cols])[0][1]
                    s_action, s_detail = ("âŒ å¤šå‡º", f"å¸³{pnl:.0f}(å‡º:{ep:.0%})") if ep > self.params['exit'] else ("â³ çºŒæŠ±", f"å¸³{pnl:.0f}(çºŒ:{1-ep:.0%})")
                    if ep > self.params['exit']: s_pos = 0
            elif s_pos == -1:
                pnl = s_price - curr_row['Close']
                if pnl <= -self.params['stop']:
                    s_pos, s_action, s_detail = 0, "ğŸ’¥ åœæ", f"æ {pnl:.0f}"
                else:
                    curr_feats_exit = curr_feats.assign(Floating_PnL=pnl, Hold_Bars=i-s_idx)
                    ep = self.models['Short_Exit_Model'].predict_proba(curr_feats_exit[self.processor.exit_feature_cols])[0][1]
                    s_action, s_detail = ("â ç©ºå‡º", f"å¸³{pnl:.0f}(å‡º:{ep:.0%})") if ep > self.params['exit'] else ("â³ çºŒæŠ±", f"å¸³{pnl:.0f}(çºŒ:{1-ep:.0%})")
                    if ep > self.params['exit']: s_pos = 0

            u_action, u_note = "-", "-"
            if u_pos != "Empty" and i >= user_entry_idx:
                hold_bars = i - user_entry_idx
                if u_pos == "Long":
                    pnl = curr_row['Close'] - user_cost
                    if i == user_entry_idx: u_action, u_note = "ğŸ”´ å¤šå–®é€²å ´", f"æœ¬ {user_cost:.0f}"
                    elif pnl <= -self.params['stop']: u_action, u_note = "ğŸ’¥ åœæ", f"{pnl:.0f}"
                    else:
                        curr_feats_exit = curr_feats.assign(Floating_PnL=pnl, Hold_Bars=hold_bars)
                        ep = self.models['Long_Exit_Model'].predict_proba(curr_feats_exit[self.processor.exit_feature_cols])[0][1]
                        u_action = "âŒ å‡ºå ´" if ep > self.params['exit'] else ("ğŸ”¥ åŠ ç¢¼" if p_long > self.params['entry'] else "â³ çºŒæŠ±")
                        u_note = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"
                elif u_pos == "Short":
                    pnl = user_cost - curr_row['Close']
                    if i == user_entry_idx: u_action, u_note = "ğŸŸ¢ ç©ºå–®é€²å ´", f"æœ¬ {user_cost:.0f}"
                    elif pnl <= -self.params['stop']: u_action, u_note = "ğŸ’¥ åœæ", f"{pnl:.0f}"
                    else:
                        curr_feats_exit = curr_feats.assign(Floating_PnL=pnl, Hold_Bars=hold_bars)
                        ep = self.models['Short_Exit_Model'].predict_proba(curr_feats_exit[self.processor.exit_feature_cols])[0][1]
                        u_action = "â å‡ºå ´" if ep > self.params['exit'] else ("ğŸ”¥ åŠ ç¢¼" if p_short > self.params['entry'] else "â³ çºŒæŠ±")
                        u_note = f"å¸³{pnl:.0f}(å‡º:{ep:.0%})"

            history_records.append({
                'Time': curr_row['Time'], 'Close': curr_row['Close'],
                'UB': curr_row.get('UB', np.nan), 'LB': curr_row.get('LB', np.nan),
                'Strategy_Action': s_action, 'Strategy_Detail': s_detail,
                'User_Advice': u_action, 'User_Note': u_note
            })
            
        return pd.DataFrame(history_records), user_entry_idx

# ==========================================
# 4. GitHub å­˜æª”åŠŸèƒ½
# ==========================================
def push_to_github(file_path, df_to_save):
    token = st.secrets.get("GITHUB_TOKEN")
    repo_name = st.secrets.get("GITHUB_REPO")
    
    if not token or not repo_name:
        return "âŒ ç¼ºå°‘ GitHub è¨­å®š"
    
    # Repo æ ¼å¼æª¢æŸ¥
    if "/" not in repo_name:
        return f"âŒ Repo åç¨±éŒ¯èª¤: '{repo_name}'ã€‚è«‹å‹™å¿…ä½¿ç”¨ 'username/repo_name' æ ¼å¼ï¼"

    try:
        g = Github(token)
        # é€™è£¡æœƒè§¸ç™¼ 404 å¦‚æœ Token æ¬Šé™ä¸å¤ æˆ– Repo ä¸å­˜åœ¨
        repo = g.get_repo(repo_name)
        
        csv_content = df_to_save.to_csv(index=False)
        try:
            contents = repo.get_contents(file_path)
            repo.update_file(contents.path, f"Update {file_path}", csv_content, contents.sha)
            return "âœ… é›²ç«¯æ›´æ–°æˆåŠŸï¼"
        except:
            repo.create_file(file_path, f"Create {file_path}", csv_content)
            return "âœ… é›²ç«¯å»ºç«‹æˆåŠŸï¼"
    except Exception as e:
        # [Fix] æ›´è©³ç´°çš„éŒ¯èª¤æç¤º
        err_msg = str(e)
        if "404" in err_msg and "Not Found" in err_msg:
            return (
                f"âŒ GitHub å›å‚³ 404 éŒ¯èª¤ (æ‰¾ä¸åˆ° Repo)ã€‚è«‹æª¢æŸ¥ï¼š\n"
                f"1. Token æ˜¯å¦å·²å‹¾é¸ 'repo' (Full control) æ¬Šé™ï¼Ÿ(ç§æœ‰åº«å¿…é ˆ)\n"
                f"2. Repo åç¨± '{repo_name}' æ˜¯å¦å®Œå…¨æ­£ç¢ºï¼Ÿ\n"
                f"3. è©² Repo æ˜¯å¦çœŸçš„å­˜åœ¨ï¼Ÿ"
            )
        return f"âŒ GitHub æ¨é€å¤±æ•—: {e}"

# ==========================================
# 5. UI ä¸»ç¨‹å¼
# ==========================================
@st.cache_resource
def load_models():
    try:
        models = {}
        for name in ['Long_Entry_Model', 'Short_Entry_Model', 'Long_Exit_Model', 'Short_Exit_Model']:
            if os.path.exists(f"models/{name}.pkl"): models[name] = joblib.load(f"models/{name}.pkl")
            elif os.path.exists(f"{name}.pkl"): models[name] = joblib.load(f"{name}.pkl")
        return models if len(models)==4 else None
    except: return None

# è‡ªå‹•åˆ·æ–°
if st_autorefresh: st_autorefresh(interval=300000, key="auto_refresh")

engine = DataEngine()
models = load_models()

with st.sidebar:
    st.header("ğŸ® æ§åˆ¶å°")
    col_day, col_full = st.columns(2)
    trigger_day = col_day.button("ğŸŒ æ›´æ–°æ—¥ç›¤", type="primary", use_container_width=True)
    trigger_full = col_full.button("ğŸŒ™ æ›´æ–°å…¨ç›¤", use_container_width=True)
    
    with st.expander("âš™ï¸ åƒæ•¸èˆ‡éƒ¨ä½", expanded=True):
        p_entry = st.slider("é€²å ´ä¿¡å¿ƒ", 0.5, 0.95, 0.80, 0.05)
        p_exit = st.slider("å‡ºå ´æ©Ÿç‡", 0.3, 0.9, 0.50, 0.05)
        p_stop = st.number_input("ç¡¬åœæ", 100, step=10)
        st.markdown("---")
        u_pos = st.radio("çœŸå¯¦æŒå€‰", ["ç©ºæ‰‹ (Empty)", "å¤šå–® (Long)", "ç©ºå–® (Short)"])
        u_time = st.time_input("é€²å ´æ™‚é–“", value=dt_time(9,0), step=300) if u_pos != "ç©ºæ‰‹ (Empty)" else None

    with st.expander("ğŸ’¾ è³‡æ–™åº«ç®¡ç†", expanded=False):
        st.caption("æ‰‹å‹•ä¸Šå‚³/ä¸‹è¼‰ CSV å‚™ä»½")
        
        tab_db_day, tab_db_full = st.tabs(["æ—¥ç›¤åº«", "å…¨ç›¤åº«"])
        
        with tab_db_day:
            up_day = st.file_uploader("ä¸Šå‚³æ—¥ç›¤æ­·å²", type=['csv'], key="up_day")
            if up_day:
                pd.read_csv(up_day).to_csv(HIST_FILE_DAY, index=False)
                st.success("å·²æ›´æ–°æ—¥ç›¤åº«")
            if st.button("å¯«å…¥ GitHub (æ—¥ç›¤)", key="git_day"):
                if os.path.exists(HIST_FILE_DAY):
                    st.write(push_to_github(HIST_FILE_DAY, pd.read_csv(HIST_FILE_DAY)))
                else: st.error("ç„¡æœ¬åœ°æª”")

        with tab_db_full:
            up_full = st.file_uploader("ä¸Šå‚³å…¨ç›¤æ­·å²", type=['csv'], key="up_full")
            if up_full:
                pd.read_csv(up_full).to_csv(HIST_FILE_FULL, index=False)
                st.success("å·²æ›´æ–°å…¨ç›¤åº«")
            if st.button("å¯«å…¥ GitHub (å…¨ç›¤)", key="git_full"):
                if os.path.exists(HIST_FILE_FULL):
                    st.write(push_to_github(HIST_FILE_FULL, pd.read_csv(HIST_FILE_FULL)))
                else: st.error("ç„¡æœ¬åœ°æª”")

def process_data(mode):
    # 1. åˆ¤æ–·è¦ç”¨çš„æ­·å²æª”
    hist_file = HIST_FILE_DAY if mode == 'day' else HIST_FILE_FULL
    
    # 2. æŠ“å– API æ–°è³‡æ–™ (é‰…äº¨ç¶²)
    api_df = engine.fetch_anue_raw()
    
    # 3. è®€å–èˆ‡åˆä½µ
    # æ³¨æ„: merge_and_save è£¡é¢æœƒè² è²¬æ—¥ç›¤éæ¿¾ & è‡ªå‹•æ¸…ç†
    final_df = engine.merge_and_save(api_df, hist_file, is_day_mode=(mode=='day'))
    
    if final_df.empty:
        return pd.DataFrame(), "ç„¡è³‡æ–™ (API å¤±æ•—ä¸”ç„¡æ­·å²æª”)"
        
    # å¦‚æœæ˜¯æ—¥ç›¤æ¨¡å¼ï¼Œä½† API æ²’çµ¦æ±è¥¿ (è¡¨ç¤ºæ”¶ç›¤äº†)ï¼Œè¦ç‰¹åˆ¥æ¨™ç¤º
    status = "OK"
    if api_df.empty:
        status = "âš ï¸ API ç„¡æ–°è³‡æ–™ï¼Œåƒ…é¡¯ç¤ºæ­·å²å­˜æª” (å¯èƒ½å·²æ”¶ç›¤)"
    
    # 4. è¨ˆç®—æŒ‡æ¨™
    df_calc = engine.calculate_indicators(final_df, mode=mode)
    
    return df_calc, status

if trigger_day:
    with st.spinner("æ•´åˆæ—¥ç›¤æ•¸æ“šä¸­..."):
        df_res, status = process_data('day')
        
        if not df_res.empty:
            st.session_state.df_view = df_res
            st.session_state.current_mode = 'day'
            st.session_state.last_update = datetime.now()
            
            # [Fix] å¦‚æœä¸æ˜¯ OKï¼Œå°± toast è­¦å‘Šä¸€ä¸‹ï¼Œä¸è¦é¡¯ç¤ºç¶ è‰²æˆåŠŸ
            if status != "OK":
                st.toast(status, icon="âš ï¸")
        else:
            st.error(status)

if trigger_full:
    with st.spinner("æ•´åˆå…¨ç›¤æ•¸æ“šä¸­..."):
        df_res, status = process_data('full')
        if not df_res.empty:
            st.session_state.df_view = df_res
            st.session_state.current_mode = 'full'
            st.session_state.last_update = datetime.now()
            if status != "OK":
                st.toast(status, icon="âš ï¸")
        else:
            st.error(status)

if not st.session_state.df_view.empty and models:
    mode_name = "ğŸŒ æ—¥ç›¤" if st.session_state.current_mode == 'day' else "ğŸŒ™ å…¨ç›¤"
    st.title(f"{mode_name}æˆ°æƒ…å®¤")
    
    # é¡¯ç¤ºè³‡æ–™åº«ç‹€æ…‹
    if st.session_state.data_range_info:
        st.info(f"ğŸ’¾ è³‡æ–™åº«ç¯„åœ (æœ€è¿‘ 5 æ—¥): {st.session_state.data_range_info}")
    
    st.caption(f"æœ€å¾Œæ›´æ–°: {st.session_state.last_update.strftime('%H:%M:%S')}")
    
    if len(st.session_state.df_view) < 50:
        st.warning(f"âš ï¸ è³‡æ–™ç­†æ•¸ ({len(st.session_state.df_view)}) ä¸è¶³ï¼ŒæŠ€è¡“æŒ‡æ¨™å¯èƒ½åå·®ã€‚")

    strat = StrategyEngine(models, {'entry': p_entry, 'exit': p_exit, 'stop': p_stop}, st.session_state.df_view)
    df_display, entry_idx = strat.run_analysis(u_pos,
